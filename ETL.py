# -*- coding: utf-8 -*-
"""Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10J2tzFovFb2bi6UKLUpzFrHU07brB95S
"""

import requests
import pandas as pd
from datetime import datetime, timedelta
!pip install isodate
import isodate

# YouTube API Key (for the purpose of this assignment)
API_KEY = "AIzaSyA1N1ho79aHEzHhG4eHWDouT8kqN8iGKPY"
YOUTUBE_TRENDING_URL = "https://www.googleapis.com/youtube/v3/videos"

def extract_trending_videos(regionCode, maxResults=10):

#Extract trending videos from YouTube API.
    params = {
        "part": "snippet,statistics,contentDetails",  # Added contentDetails for duration
        "chart": "mostPopular",
        "regionCode": regionCode,
        "maxResults": maxResults,
        "key": API_KEY
    }

    response = requests.get(YOUTUBE_TRENDING_URL, params=params)

    if response.status_code == 200:
        return response.json()
    else:
        print(f"Error: Unable to fetch data. Status Code: {response.status_code}")
        return None

def transform_data(json_data):

#Transform raw JSON data into a structured DataFrame.

    if not json_data or "items" not in json_data:
        print("No data available for transformation.")
        return pd.DataFrame()

    video_list = []

    for item in json_data["items"]:
        video_id = item["id"]
        title = item["snippet"]["title"]
        channel_title = item["snippet"]["channelTitle"]
        published_at = item["snippet"]["publishedAt"]
        views = int(item["statistics"].get("viewCount", 0))
        likes = int(item["statistics"].get("likeCount", 0))
        comments = int(item["statistics"].get("commentCount", 0))
        category_id = item["snippet"]["categoryId"]

# Extract video duration (ISO 8601 format -> HH:MM:SS)
        iso_duration = item["contentDetails"]["duration"]
        video_duration = str(isodate.parse_duration(iso_duration))  # Convert to HH:MM:SS format

# Calculate how many days and hours since the video was published
        published_date = datetime.strptime(published_at, "%Y-%m-%dT%H:%M:%SZ")
        time_diff = datetime.utcnow() - published_date
        days_since_published = time_diff.days
        hours_since_published = time_diff.seconds // 3600  # Convert seconds to hours

        video_list.append({
            "Video ID": video_id,
            "Title": title,
            "Channel": channel_title,
            "Published At": published_at,
            "Days Since Published": days_since_published,
            "Hours Since Published": hours_since_published,
            "Views": views,
            "Likes": likes,
            "Comments": comments,
            "Video Duration": video_duration,  # HH:MM:SS format

        })

    return pd.DataFrame(video_list)

def youtube_etl(regionCode, maxResults=10):

    print(f"Fetching top {maxResults} trending videos for region: {regionCode}")

# Extract
    raw_data = extract_trending_videos(regionCode, maxResults)

# Transform
    df = transform_data(raw_data)

# Load (return DataFrame)
    return df

# Example usage (Fetching trending videos in India ðŸ‡®ðŸ‡³)
df_trending = youtube_etl(regionCode="IN", maxResults=10)
print(df_trending)
df_trending.to_csv("trending_videos.tsv", sep="\t", index=False) #SAVING THE TSV FILE